# Internal Data Engineering Knowledge RAG Pipeline
## ğŸ“Œ Overview

This project is a Retrieval-Augmented Generation (RAG) pipeline designed to make internal Data Engineering documentation, Python scripts, and dbt models easily accessible through a Large Language Model (LLM).

By parsing and indexing internal resources, the system allows team members to query the companyâ€™s technical knowledge base in natural language, improving onboarding, troubleshooting, and knowledge sharing.

## âš™ï¸ Features

Document Parsing â€“ Extracts content from internal documents (e.g., markdown, Word, Google Docs exports).

Code Indexing â€“ Parses documented Python scripts and dbt models for searchability.

RAG Architecture â€“ Combines a vector search index with an LLM for context-aware responses.

Multi-Source Support â€“ Handles a mix of text documents, codebases, and dbt SQL models.

Searchable Knowledge Base â€“ Allows semantic search across all indexed content.

## ğŸ› ï¸ Tech Stack

Programming Language: Python

Vector Store: FAISS

Embedding Model: HuggingFace embeddings

LLM: mistral-7b

Parsing: LangChain document loaders & custom parsers

Orchestration: Python scripts


## ğŸ“‚ Project Structure

â”œâ”€â”€ data/                     # Raw exported documents & scripts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings.py         # Parses and indexes data
â”‚   â”œâ”€â”€ query_pipeline.py     # Queries the vector DB via LLM
â””â”€â”€ README.md    


## ğŸš€ How It Works

Ingestion & Parsing â€“ Reads internal docs, Python scripts, and dbt SQL files, then cleans and normalizes them.

Embedding & Indexing â€“ Converts content into vector embeddings and stores them in a vector database.

Query Processing â€“ Accepts a natural language question, retrieves the most relevant indexed content, and sends it to the LLM.

Context-Aware Answering â€“ The LLM uses retrieved context to generate accurate, source-backed responses.

## ğŸ“ˆ Use Cases

Speeding up onboarding for new data engineers

Reducing manual searching through internal docs

Centralizing tribal knowledge into one searchable system

Supporting LLM-powered assistants for technical Q&A
